{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This particular assignment focuses on text classification using CNN. It has been picking up pace over the past few years. So, I thought this would be a good exercise to try out. The dataset is provided to you and there will be specific instrucions on how to curate the data, split into train and validation and the like.  You will be using MXnet for this task.  The data comprises tweets pertaining to common causes of cancer. The objective is to classify the tweets as medically relevant or not.  The dataset is skewed with positive class or 'yes' being 6 times less frequent than the negative class or 'no'. (Total marks = 50). Individual marks to the sub-problems are given in bracket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208 1298\n"
     ]
    }
   ],
   "source": [
    "# these are the modules you are allowed to work with. \n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import sys, os\n",
    "\n",
    "'''\n",
    "First job is to clean and preprocess the social media text. (5)\n",
    "\n",
    "1) Replace URLs and mentions (i.e strings which are preceeded with @)\n",
    "2) Segment #hastags \n",
    "3) Remove emoticons and other unicode characters\n",
    "'''\n",
    "\n",
    "def preprocess_tweet(input_text):\n",
    "    '''\n",
    "    Input: The input string read directly from the file\n",
    "    \n",
    "    Output: Pre-processed tweet text\n",
    "    '''\n",
    "    urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+] |[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', input_text)\n",
    "    for url in urls:\n",
    "        input_text.replace(url,\"\")\n",
    "    input_text.replace(\"@\",\"\")\n",
    "    list = input_text.split(\" \")\n",
    "    for word in list:\n",
    "        splitted = re.sub('(?!^)([A-Z][a-z]+)', r' \\1', word).split()\n",
    "        strn =\"\"\n",
    "        for i in splitted:\n",
    "          strn = strn+i+\" \"\n",
    "        input_text.replace(word , strn)\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', input_text)\n",
    "    return input_text\n",
    "\n",
    "\n",
    "\n",
    "# read the input file and create the set of positive examples and negative examples. \n",
    "\n",
    "file=open('cancer_data.tsv')\n",
    "pos_data=[]\n",
    "neg_data=[]\n",
    "\n",
    "for line in file:\n",
    "    line=line.strip().split('\\t')\n",
    "    text2= preprocess_tweet(line[0]).strip().split()\n",
    "    if line[1]=='yes':\n",
    "        pos_data.append(text2)\n",
    "    if line[1]=='no':\n",
    "        neg_data.append(text2)\n",
    "\n",
    "print(len(pos_data), len(neg_data))     \n",
    "\n",
    "sentences= list(pos_data)\n",
    "sentences.extend(neg_data)\n",
    "pos_labels= [1 for _ in pos_data]\n",
    "neg_labels= [0 for _ in neg_data]\n",
    "y=list(pos_labels)\n",
    "y.extend(neg_labels)\n",
    "y=np.array(y)\n",
    "\n",
    "'''\n",
    "After this you will obtain the following :\n",
    "\n",
    "1) sentences =  List of sentences having the positive and negative examples with all the positive examples first\n",
    "2) y = List of labels with the positive labels first.\n",
    "'''\n",
    "\n",
    "'''\n",
    "Before running the CNN there are a few things one needs to take care of: (5)\n",
    "\n",
    "1) Pad the sentences so that all of them are of the same length\n",
    "2) Build a vocabulary comprising all unique words that occur in the corpus\n",
    "3) Convert each sentence into a corresponding vector by replacing each word in the sentence with the index in the vocabulary. \n",
    "\n",
    "Example :\n",
    "S1 = a b a c\n",
    "S2 = d c a \n",
    "\n",
    "Step 1:  S1= a b a c, \n",
    "         S2 =d c a </s> \n",
    "         (Both sentences are of equal length). \n",
    "\n",
    "Step 2:  voc={a:1, b:2, c:3, d:4, </s>: 5}\n",
    "\n",
    "Step 3:  S1= [1,2,1,3]\n",
    "         S2= [4,3,1,5]\n",
    "\n",
    "'''\n",
    "\n",
    "def create_word_vectors(sentences):\n",
    "    '''\n",
    "    Input: List of sentences\n",
    "    Output: List of word vectors corresponding to each sentence, vocabulary\n",
    "    '''\n",
    "    maxSentLen = 0\n",
    "    for i in sentences:\n",
    "       if maxSentLen<len(i):\n",
    "        maxSentLen=len(i)\n",
    "    for i in sentences:\n",
    "        if(len(i)<maxSentLen):\n",
    "            for c in range(maxSentLen-len(i)):\n",
    "                i.append(\"</s>\")\n",
    "\n",
    "    vocabulary = set()\n",
    "    for i in sentences:\n",
    "        for j in i:\n",
    "            vocabulary.add(j)\n",
    "    vocabulary = list(vocabulary)\n",
    "    word_vectors=[]\n",
    "    for sent in sentences:\n",
    "        sentVector = []\n",
    "        for word in sent:\n",
    "            sentVector.append(vocabulary.index(word))\n",
    "        word_vectors.append(sentVector)\n",
    "        \n",
    "    \n",
    "    return word_vectors, vocabulary\n",
    "\n",
    "\n",
    "x, vocabulary = create_word_vectors(sentences)\n",
    "vocabSize = len(vocabulary)\n",
    "maxSentLen = len(x[0])\n",
    "\n",
    "\n",
    "def create_shuffle(x,y):\n",
    "    '''\n",
    "    Create an equal distribution of the positive and negative examples. \n",
    "    Please do not change this particular shuffling method.\n",
    "    '''\n",
    "    pos_len= len(pos_data)\n",
    "    neg_len= len(neg_data)\n",
    "    pos_len_train= int(0.8*pos_len)\n",
    "    neg_len_train= int(0.8*neg_len)\n",
    "    train_data= [(x[i],y[i]) for i in range(0, pos_len_train)]\n",
    "    train_data.extend([(x[i],y[i]) for i in range(pos_len, pos_len+ neg_len_train )])\n",
    "    test_data=[(x[i],y[i]) for i in range(pos_len_train, pos_len)]\n",
    "    test_data.extend([(x[i],y[i]) for i in range(pos_len+ neg_len_train, len(x) )])\n",
    "    \n",
    "    import random\n",
    "    random.shuffle(train_data)\n",
    "    x_train=[i[0] for i in train_data]\n",
    "    y_train=[i[1] for i in train_data]\n",
    "    random.shuffle(test_data)\n",
    "    x_test=[i[0] for i in test_data]\n",
    "    y_test=[i[1] for i in test_data]\n",
    "    \n",
    "    x_train=np.array(x_train)\n",
    "    y_train=np.array(y_train)\n",
    "    x_test= np.array(x_test)\n",
    "    y_test= np.array(y_test)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_test, y_test= create_shuffle(x,y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer rmsprop\n",
      "maximum gradient 5.0\n",
      "learning rate (step size) 0.005\n",
      "epochs to train for 10\n",
      "Iter [0] Train: Time: 13.828s, Training Accuracy: 85.833             --- Dev Accuracy thus far: 88.667\n",
      "Iter [1] Train: Time: 13.742s, Training Accuracy: 97.333             --- Dev Accuracy thus far: 88.000\n",
      "Iter [2] Train: Time: 13.891s, Training Accuracy: 99.333             --- Dev Accuracy thus far: 89.667\n",
      "Iter [3] Train: Time: 13.731s, Training Accuracy: 99.750             --- Dev Accuracy thus far: 90.333\n",
      "Iter [4] Train: Time: 13.816s, Training Accuracy: 99.750             --- Dev Accuracy thus far: 90.000\n",
      "Iter [5] Train: Time: 13.824s, Training Accuracy: 100.000             --- Dev Accuracy thus far: 90.000\n",
      "Iter [6] Train: Time: 13.769s, Training Accuracy: 100.000             --- Dev Accuracy thus far: 90.000\n",
      "Iter [7] Train: Time: 13.698s, Training Accuracy: 100.000             --- Dev Accuracy thus far: 89.667\n",
      "Iter [8] Train: Time: 13.932s, Training Accuracy: 100.000             --- Dev Accuracy thus far: 89.667\n",
      "Saved checkpoint to ./cnn-0009.params\n",
      "Iter [9] Train: Time: 13.764s, Training Accuracy: 100.000             --- Dev Accuracy thus far: 89.667\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "We now define the neural architecture of the CNN. The architecture is defined as : (10)\n",
    "\n",
    "1) Embedding layer that converts the vector representation of the sentence from a one-hot encoding to a fixed sized word embedding\n",
    "   (mx.sym.Embedding)\n",
    "   \n",
    "2) Convolution + activation + max pooling layer \n",
    "   (mx.sym.Convolution+ mx.sym.Activation+ mx.sym.Pooling)\n",
    "   This procedure is to be followed for different sizes of filters (the filters corresponding to size 2 looks at the bigram distribution, 3 looks at trigram etc. \n",
    "\n",
    "3) Concat all the filters together (mx.sym.Concat)\n",
    "\n",
    "4) Pass the results through a fully Connected layer of size 2 and then run softmax on it. \n",
    "   (mx.sym.FullyConnected, mx.sym.SoftmaxOutput)\n",
    "   \n",
    "\n",
    "We then initialize the intermediate layers of appropriate size and train the model using back prop. (10)\n",
    "(Look up the mxnet tutorial if you have any doubt)\n",
    "\n",
    "Run the classifier and for each epoch with a specified batch size observe the accuracy on the training set and test set (5)\n",
    "\n",
    "\n",
    "Default parameters:\n",
    "\n",
    "1) No of epochs = 10\n",
    "2) Batch size = 20\n",
    "3) Size of word embeddings = 200\n",
    "4) Size of filters =[2,3,4,5]\n",
    "5) Filter embedding= 100\n",
    "6) Optimizer = rmsprop\n",
    "7) learning rate = 0.005\n",
    "\n",
    "'''\n",
    "batchSize = 20\n",
    "wordEmbSize = 200\n",
    "filterList = [2,3,4,5]\n",
    "numFilters = 100\n",
    "input_x = mx.sym.Variable('data') # placeholder for input data\n",
    "input_y = mx.sym.Variable('softmax_label') # placeholder for output label\n",
    "embed_layer = mx.sym.Embedding(data=input_x, input_dim=vocabSize, output_dim=wordEmbSize, name='vocab_embed')\n",
    "conv_input = mx.sym.Reshape(data=embed_layer, shape=(batchSize, 1, maxSentLen, wordEmbSize))\n",
    "pooled_outputs = []\n",
    "for filter_size in filterList:\n",
    "    convi = mx.sym.Convolution(data=conv_input, kernel=(filter_size, wordEmbSize), num_filter=numFilters)\n",
    "    relui = mx.sym.Activation(data=convi, act_type='relu')\n",
    "    pooli = mx.sym.Pooling(data=relui, pool_type='max', kernel=(maxSentLen - filter_size + 1, 1), stride=(1, 1))\n",
    "    pooled_outputs.append(pooli)\n",
    "\n",
    "# combine all pooled outputs\n",
    "total_filters = numFilters * len(filterList)\n",
    "concat = mx.sym.Concat(*pooled_outputs, dim=1)\n",
    "\n",
    "# reshape for next layer\n",
    "h_pool = mx.sym.Reshape(data=concat, shape=(batchSize, total_filters))\n",
    "\n",
    "num_label = 2\n",
    "\n",
    "cls_weight = mx.sym.Variable('cls_weight')\n",
    "cls_bias = mx.sym.Variable('cls_bias')\n",
    "\n",
    "fc = mx.sym.FullyConnected(data=h_pool, weight=cls_weight, bias=cls_bias, num_hidden=num_label)\n",
    "\n",
    "# softmax output\n",
    "sm = mx.sym.SoftmaxOutput(data=fc, label=input_y, name='softmax')\n",
    "\n",
    "# set CNN pointer to the \"back\" of the network\n",
    "cnn = sm\n",
    "\n",
    "\n",
    "from collections import namedtuple\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Define the structure of our CNN Model (as a named tuple)\n",
    "CNNModel = namedtuple(\"CNNModel\", ['cnn_exec', 'symbol', 'data', 'label', 'param_blocks'])\n",
    "\n",
    "# Define what device to train/test on, use GPU if available\n",
    "ctx = mx.gpu() if mx.test_utils.list_gpus() else mx.cpu()\n",
    "\n",
    "arg_names = cnn.list_arguments()\n",
    "\n",
    "input_shapes = {}\n",
    "input_shapes['data'] = (batchSize, maxSentLen)\n",
    "\n",
    "arg_shape, out_shape, aux_shape = cnn.infer_shape(**input_shapes)\n",
    "arg_arrays = [mx.nd.zeros(s, ctx) for s in arg_shape]\n",
    "args_grad = {}\n",
    "for shape, name in zip(arg_shape, arg_names):\n",
    "    if name in ['softmax_label', 'data']: # input, output\n",
    "        continue\n",
    "    args_grad[name] = mx.nd.zeros(shape, ctx)\n",
    "\n",
    "cnn_exec = cnn.bind(ctx=ctx, args=arg_arrays, args_grad=args_grad, grad_req='add')\n",
    "\n",
    "param_blocks = []\n",
    "arg_dict = dict(zip(arg_names, cnn_exec.arg_arrays))\n",
    "initializer = mx.initializer.Uniform(0.1)\n",
    "for i, name in enumerate(arg_names):\n",
    "    if name in ['softmax_label', 'data']: # input, output\n",
    "        continue\n",
    "    initializer(mx.init.InitDesc(name), arg_dict[name])\n",
    "\n",
    "    param_blocks.append( (i, arg_dict[name], args_grad[name], name) )\n",
    "\n",
    "data = cnn_exec.arg_dict['data']\n",
    "label = cnn_exec.arg_dict['softmax_label']\n",
    "\n",
    "cnn_model= CNNModel(cnn_exec=cnn_exec, symbol=cnn, data=data, label=label, param_blocks=param_blocks)\n",
    "\n",
    "optimizer = 'rmsprop'\n",
    "max_grad_norm = 5.0\n",
    "learning_rate = 0.005\n",
    "epoch = 10\n",
    "\n",
    "print('optimizer', optimizer)\n",
    "print('maximum gradient', max_grad_norm)\n",
    "print('learning rate (step size)', learning_rate)\n",
    "print('epochs to train for', epoch)\n",
    "\n",
    "# create optimizer\n",
    "opt = mx.optimizer.create(optimizer)\n",
    "opt.lr = learning_rate\n",
    "\n",
    "updater = mx.optimizer.get_updater(opt)\n",
    "\n",
    "# For each training epoch\n",
    "for iteration in range(epoch):\n",
    "    tic = time.time()\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "\n",
    "    # Over each batch of training data\n",
    "    for begin in range(0, x_train.shape[0], batchSize):\n",
    "        batchX = x_train[begin:begin+batchSize]\n",
    "        batchY = y_train[begin:begin+batchSize]\n",
    "        if batchX.shape[0] != batchSize:\n",
    "            continue\n",
    "\n",
    "        cnn_model.data[:] = batchX\n",
    "        cnn_model.label[:] = batchY\n",
    "\n",
    "        # forward\n",
    "        cnn_model.cnn_exec.forward(is_train=True)\n",
    "\n",
    "        # backward\n",
    "        cnn_model.cnn_exec.backward()\n",
    "\n",
    "        # eval on training data\n",
    "        num_correct += sum(batchY == np.argmax(cnn_model.cnn_exec.outputs[0].asnumpy(), axis=1))\n",
    "        num_total += len(batchY)\n",
    "\n",
    "        # update weights\n",
    "        norm = 0\n",
    "        for idx, weight, grad, name in cnn_model.param_blocks:\n",
    "            grad /= batchSize\n",
    "            l2_norm = mx.nd.norm(grad).asscalar()\n",
    "            norm += l2_norm * l2_norm\n",
    "\n",
    "        norm = math.sqrt(norm)\n",
    "        for idx, weight, grad, name in cnn_model.param_blocks:\n",
    "            if norm > max_grad_norm:\n",
    "                grad *= (max_grad_norm / norm)\n",
    "\n",
    "            updater(idx, grad, weight)\n",
    "\n",
    "            # reset gradient to zero\n",
    "            grad[:] = 0.0\n",
    "\n",
    "    # Decay learning rate for this epoch to ensure we are not \"overshooting\" optima\n",
    "    if iteration % 50 == 0 and iteration > 0:\n",
    "        opt.lr *= 0.5\n",
    "        print('reset learning rate to %g' % opt.lr)\n",
    "\n",
    "    # End of training loop for this epoch\n",
    "    toc = time.time()\n",
    "    train_time = toc - tic\n",
    "    train_acc = num_correct * 100 / float(num_total)\n",
    "\n",
    "    # Saving checkpoint to disk\n",
    "    if (iteration + 1) % 10 == 0:\n",
    "        prefix = 'cnn'\n",
    "        cnn_model.symbol.save('./%s-symbol.json' % prefix)\n",
    "        save_dict = {('arg:%s' % k) : v  for k, v in cnn_model.cnn_exec.arg_dict.items()}\n",
    "        save_dict.update({('aux:%s' % k) : v for k, v in cnn_model.cnn_exec.aux_dict.items()})\n",
    "        param_name = './%s-%04d.params' % (prefix, iteration)\n",
    "        mx.nd.save(param_name, save_dict)\n",
    "        print('Saved checkpoint to %s' % param_name)\n",
    "\n",
    "\n",
    "    # Evaluate model after this epoch on dev (test) set\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "\n",
    "    # For each test batch\n",
    "    for begin in range(0, x_test.shape[0], batchSize):\n",
    "        batchX = x_test[begin:begin+batchSize]\n",
    "        batchY = y_test[begin:begin+batchSize]\n",
    "\n",
    "        if batchX.shape[0] != batchSize:\n",
    "            continue\n",
    "\n",
    "        cnn_model.data[:] = batchX\n",
    "        cnn_model.cnn_exec.forward(is_train=False)\n",
    "\n",
    "        num_correct += sum(batchY == np.argmax(cnn_model.cnn_exec.outputs[0].asnumpy(), axis=1))\n",
    "        num_total += len(batchY)\n",
    "\n",
    "    dev_acc = num_correct * 100 / float(num_total)\n",
    "    print('Iter [%d] Train: Time: %.3fs, Training Accuracy: %.3f \\\n",
    "            --- Test Accuracy thus far: %.3f' % (iteration, train_time, train_acc, dev_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nSo far, the assignment has been posed in a manner so that you can refer to directly the mxnet tutorial on the same problem. \\n\\nThe final 15 marks is meant to carry out experimentations of your own and observe how the results change by experimentation. \\n\\n1) Would the results improve if instead of using the word embeddings that is based solely on frequency, if you have been able to incorporate sub-word information\\n   (In short run fasttext on the corpus and use the word embeddings generated by fastetxt). (8)\\n   \\n2) Accuracy might not be the best way to measure the performance of a skewed dataset. What other metrics would you use ? Why? \\n   Experiment with different hyper-paramters to show the performance in terms of metric? \\n   You can assume that we want to identify all the medically relevant tweets (i.e. tweets with 'yes' class more). (7)\\n    \\n\\nDelivearbles:\\n\\nThe ipython notebook with the results to each part of the question. \\n\\n\\nP.S: This assignment is part of a research question I am working on my free time. So if you have any insights, I'd love to hear them. \\nHappy coding \\n\\nRitam Dutt\\n14CS30041\\n\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "So far, the assignment has been posed in a manner so that you can refer to directly the mxnet tutorial on the same problem. \n",
    "\n",
    "The final 15 marks is meant to carry out experimentations of your own and observe how the results change by experimentation. \n",
    "\n",
    "1) Would the results improve if instead of using the word embeddings that is based solely on frequency, if you have been able to incorporate sub-word information\n",
    "   (In short run fasttext on the corpus and use the word embeddings generated by fastetxt). (8)\n",
    "   \n",
    "2) Accuracy might not be the best way to measure the performance of a skewed dataset. What other metrics would you use ? Why? \n",
    "   Experiment with different hyper-paramters to show the performance in terms of metric? \n",
    "   You can assume that we want to identify all the medically relevant tweets (i.e. tweets with 'yes' class more). (7)\n",
    "    \n",
    "\n",
    "Delivearbles:\n",
    "\n",
    "The ipython notebook with the results to each part of the question. \n",
    "\n",
    "\n",
    "P.S: This assignment is part of a research question I am working on my free time. So if you have any insights, I'd love to hear them. \n",
    "Happy coding \n",
    "\n",
    "Ritam Dutt\n",
    "14CS30041\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fasttext'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c5c00c7017fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fasttext'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "\n",
    "import fasttext\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
